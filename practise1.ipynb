{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acc91b3",
   "metadata": {},
   "source": [
    "## PySpark Interview Questions â€“ Dataset Recap & Exercises\n",
    "\n",
    "### ðŸ“‚ Dataset Recap\n",
    "\n",
    "**customers** â†’ customer_id, name, email, city, signup_date  \n",
    "**products** â†’ product_id, name, category, price  \n",
    "**orders** â†’ order_id, customer_id, order_date, status  \n",
    "**order_items** â†’ item_id, order_id, product_id, quantity  \n",
    "**payments** â†’ payment_id, order_id, amount, payment_date, payment_method  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Beginner / Core Questions\n",
    "\n",
    "1. Read all five CSV files into PySpark DataFrames with headers and schema inference.  \n",
    "2. Show the schema of each DataFrame.  \n",
    "3. Get the total number of customers, products, and orders.  \n",
    "4. List all distinct product categories.  \n",
    "5. Find the top 5 cities with the most customers.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Joins & Aggregations\n",
    "\n",
    "6. Join orders with customers â†’ Get a DataFrame of orders with customer names.  \n",
    "7. Find the total sales per product category.  \n",
    "8. Find the average order value per customer.  \n",
    "9. Get the top 3 customers by total spending.  \n",
    "10. Find the top 5 products by total quantity sold.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Date & Time Functions\n",
    "\n",
    "11. Find the number of orders placed each month.  \n",
    "12. Get customers who signed up in 2023 but have not placed any orders.  \n",
    "13. Find the first order date for each customer.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Window Functions\n",
    "\n",
    "14. For each customer, rank their orders by order_date.  \n",
    "15. For each product, find the top 2 orders with the highest quantity.  \n",
    "16. Calculate the running total of payments per customer.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Null Handling & Data Cleaning\n",
    "\n",
    "17. Find all rows where email is null in customers.  \n",
    "18. Replace null payment_method values with `\"Unknown\"`.  \n",
    "19. Drop any orders that have no items in order_items.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Performance / Advanced\n",
    "\n",
    "20. Explain the difference between `repartition()` and `coalesce()`.  \n",
    "21. If one table is very small (like products), how can you optimize joins with large tables?  \n",
    "22. How does caching help if the same DataFrame is reused multiple times?  \n",
    "23. Whatâ€™s the difference between wide and narrow transformations? Give examples.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61112082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/19 14:36:43 WARN Utils: Your hostname, Harishankars-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.114 instead (on interface en0)\n",
      "26/01/19 14:36:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 14:36:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/19 14:36:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# use yarn as resource manager\n",
    "spark = SparkSession.builder.appName('Test App').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd7193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.110:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12adef5f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009ba4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = spark.conf.get('spark.master')\n",
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4515f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = spark.read.csv(r'/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/customers.csv', header=True, inferSchema=True)\n",
    "item_df = spark.read.csv(r'/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/items.csv', header=True, inferSchema=True)\n",
    "order_df = spark.read.csv(r'/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/orders.csv', header=True, inferSchema=True)\n",
    "payment_df = spark.read.csv(r'/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/payments.csv', header=True, inferSchema=True)\n",
    "product_df = spark.read.csv(r'/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/products.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c61868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d269694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89618f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c72d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|      Books|\n",
      "|  Furniture|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all distinct product categories.\n",
    "product_df.select('category').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79675861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+\n",
      "|customer_id| cust_name|     city|signup_date|\n",
      "+-----------+----------+---------+-----------+\n",
      "|          1|Customer_1|Bangalore| 2023-07-16|\n",
      "|          2|Customer_2|   Mumbai| 2023-01-12|\n",
      "|          3|Customer_3|  Chennai| 2023-03-05|\n",
      "|          4|Customer_4|Bangalore| 2023-01-07|\n",
      "|          5|Customer_5|  Kolkata| 2023-02-01|\n",
      "+-----------+----------+---------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "cust_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35fc7796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     city|no_of_cust|\n",
      "+---------+----------+\n",
      "|Bangalore|         6|\n",
      "|  Kolkata|         5|\n",
      "|  Chennai|         3|\n",
      "|   Mumbai|         3|\n",
      "|    Delhi|         3|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 cities with the most customers.\n",
    "from pyspark.sql.functions import col, desc\n",
    "cust_df.groupBy(col('city')).count().withColumnRenamed('count', 'no_of_cust').orderBy(desc('no_of_cust')).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12f98de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+--------+----------+---------+\n",
      "|customer_id|      name|     city|signup_date|order_id|order_date|   status|\n",
      "+-----------+----------+---------+-----------+--------+----------+---------+\n",
      "|          2|Customer_2|   Mumbai| 2023-01-12|    1014|2023-12-08|  Shipped|\n",
      "|          5|Customer_5|  Kolkata| 2023-02-01|    1017|2023-09-15|  Pending|\n",
      "|          6|Customer_6|Bangalore| 2023-08-02|    1008|2023-08-25|  Pending|\n",
      "|          6|Customer_6|Bangalore| 2023-08-02|    1001|2023-10-05|Cancelled|\n",
      "|          7|Customer_7|Bangalore| 2023-07-30|    1020|2023-03-28|Delivered|\n",
      "+-----------+----------+---------+-----------+--------+----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Join orders with customers â†’ Get a DataFrame of orders with customer names.\n",
    "cust_order_df = cust_df.join(order_df, how='inner', on='customer_id')\n",
    "cust_order_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173b52c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|   category|total_sales|\n",
      "+-----------+-----------+\n",
      "|Electronics|      93799|\n",
      "|   Clothing|     307004|\n",
      "|      Books|     227885|\n",
      "|  Furniture|     424193|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total sales per product category.\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "product_df.groupBy(col('category')).agg(_sum('price').alias('total_sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f21459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = cust_df.withColumnRenamed('name', 'cust_name')\n",
    "cust_order_amount_df = cust_df.join(order_df, how='inner', on='customer_id')\\\n",
    "                              .join(order_item_df, how='inner', on='order_id')\\\n",
    "                              .join(product_df, how='inner', on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99db1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- cust_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_order_amount_df = cust_order_amount_df.withColumnRenamed('name', 'product_name')\n",
    "cust_order_amount_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcb9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|customer_id|average_order_value|\n",
      "+-----------+-------------------+\n",
      "|          2|            32583.0|\n",
      "|          5|            77375.0|\n",
      "|          6|            53233.0|\n",
      "|          7|            47073.5|\n",
      "|          8|            43770.0|\n",
      "|          9|            30207.5|\n",
      "|         10|            57087.5|\n",
      "|         11|            60402.0|\n",
      "|         12|            53973.5|\n",
      "|         13|            47292.5|\n",
      "|         15|           26884.75|\n",
      "|         16|            51340.5|\n",
      "|         18|            57723.0|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average order value per customer.\n",
    "filtered_cust_order_amount_df = cust_order_amount_df.select('customer_id', 'cust_name', 'product_name', 'price')\n",
    "from pyspark.sql.functions import avg\n",
    "filtered_cust_order_amount_df.groupBy('customer_id').agg(avg('price').alias('average_order_value')).orderBy('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe856359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|total_order_amount|\n",
      "+-----------+------------------+\n",
      "|         12|            323841|\n",
      "|         11|            241608|\n",
      "|         18|            230892|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the top 3 customers by total spending.\n",
    "filtered_cust_order_amount_df.groupBy('customer_id').agg(_sum('price').alias('total_order_amount')).orderBy(desc('total_order_amount')).limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2de97b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+----------+-------+-----------+----------+-------+-------------+--------+------------+---------+-----+\n",
      "|product_id|order_id|customer_id| cust_name|   city|signup_date|order_date| status|order_item_id|quantity|product_name| category|price|\n",
      "+----------+--------+-----------+----------+-------+-----------+----------+-------+-------------+--------+------------+---------+-----+\n",
      "|       114|    1014|          2|Customer_2| Mumbai| 2023-01-12|2023-12-08|Shipped|           28|       3|  Product_14|Furniture|59786|\n",
      "|       112|    1014|          2|Customer_2| Mumbai| 2023-01-12|2023-12-08|Shipped|           27|       3|  Product_12|    Books| 5380|\n",
      "|       102|    1017|          5|Customer_5|Kolkata| 2023-02-01|2023-09-15|Pending|           34|       4|   Product_2|Furniture|77711|\n",
      "+----------+--------+-----------+----------+-------+-----------+----------+-------+-------------+--------+------------+---------+-----+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "cust_order_amount_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec177755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      name|total_quantity_sold|\n",
      "+----------+-------------------+\n",
      "|Product_15|                 16|\n",
      "| Product_2|                 13|\n",
      "| Product_5|                 13|\n",
      "|Product_19|                 10|\n",
      "|Product_17|                 10|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the top 5 products by total quantity sold.\n",
    "prduct_order_df = order_item_df.join(product_df, how='left', on='product_id')\n",
    "prduct_order_df.groupBy('name').agg(_sum('quantity').alias('total_quantity_sold')).orderBy(desc('total_quantity_sold')).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bafeaeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|month(order_date)|count|\n",
      "+-----------------+-----+\n",
      "|               12|    2|\n",
      "|                6|    1|\n",
      "|                3|    4|\n",
      "|                9|    2|\n",
      "|                4|    4|\n",
      "|                8|    2|\n",
      "|               10|    2|\n",
      "|               11|    1|\n",
      "|                2|    2|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the number of orders placed each month.\n",
    "from pyspark.sql.functions import day, month, year\n",
    "\n",
    "order_df.groupBy(month('order_date')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6f49b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- cust_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6355c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-----------+\n",
      "|customer_id|  cust_name|     city|signup_date|\n",
      "+-----------+-----------+---------+-----------+\n",
      "|          1| Customer_1|Bangalore| 2023-07-16|\n",
      "|          3| Customer_3|  Chennai| 2023-03-05|\n",
      "|          4| Customer_4|Bangalore| 2023-01-07|\n",
      "|         14|Customer_14|    Delhi| 2023-06-03|\n",
      "|         17|Customer_17|    Delhi| 2023-06-21|\n",
      "|         19|Customer_19|  Kolkata| 2023-06-22|\n",
      "|         20|Customer_20|    Delhi| 2023-09-15|\n",
      "+-----------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get customers who signed up in 2023 but have not placed any orders.\n",
    "cust_df.join(order_df, how='leftanti', on='customer_id').filter(year('signup_date') == 2023).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7135557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f60f5445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|min(order_date)|\n",
      "+-----------+---------------+\n",
      "|         12|     2023-02-28|\n",
      "|         13|     2023-03-29|\n",
      "|          6|     2023-08-25|\n",
      "|         16|     2023-08-01|\n",
      "|          5|     2023-09-15|\n",
      "|         15|     2023-06-27|\n",
      "|          9|     2023-04-05|\n",
      "|          8|     2023-02-20|\n",
      "|          7|     2023-03-28|\n",
      "|         10|     2023-04-17|\n",
      "|         11|     2023-03-12|\n",
      "|          2|     2023-12-08|\n",
      "|         18|     2023-04-11|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the first order date for each customer.\n",
    "from pyspark.sql.functions import min as _min\n",
    "order_df.groupBy('customer_id').agg(_min(col('order_date'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ff8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+---------+-------+\n",
      "|order_id|customer_id|order_date|   status|ranking|\n",
      "+--------+-----------+----------+---------+-------+\n",
      "|    1014|          2|2023-12-08|  Shipped|      1|\n",
      "|    1017|          5|2023-09-15|  Pending|      1|\n",
      "|    1008|          6|2023-08-25|  Pending|      1|\n",
      "|    1001|          6|2023-10-05|Cancelled|      2|\n",
      "|    1020|          7|2023-03-28|Delivered|      1|\n",
      "|    1004|          8|2023-02-20|Cancelled|      1|\n",
      "|    1016|          8|2023-04-28|Delivered|      2|\n",
      "|    1012|          9|2023-04-05|Delivered|      1|\n",
      "|    1015|         10|2023-04-17|Delivered|      1|\n",
      "|    1002|         11|2023-03-12|  Pending|      1|\n",
      "|    1019|         11|2023-11-09|  Shipped|      2|\n",
      "|    1013|         12|2023-02-28|Cancelled|      1|\n",
      "|    1010|         12|2023-03-13|  Pending|      2|\n",
      "|    1009|         12|2023-09-12|Delivered|      3|\n",
      "|    1011|         13|2023-03-29|Delivered|      1|\n",
      "|    1007|         15|2023-06-27|  Pending|      1|\n",
      "|    1006|         15|2023-12-31|  Pending|      2|\n",
      "|    1018|         16|2023-08-01|  Pending|      1|\n",
      "|    1003|         18|2023-04-11|  Pending|      1|\n",
      "|    1005|         18|2023-10-17|Delivered|      2|\n",
      "+--------+-----------+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each customer, rank their orders by order_date.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "windowSpec = Window.partitionBy('customer_id').orderBy('order_date')\n",
    "order_df.withColumn(\n",
    "    'ranking',\n",
    "     F.rank().over(windowSpec)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "589491e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each product, find the top 2 orders with the highest quantity.\n",
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32591949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+---------+----------+------------+------+\n",
      "|order_id|customer_id|order_date|   status|payment_id|payment_type|amount|\n",
      "+--------+-----------+----------+---------+----------+------------+------+\n",
      "|    1001|          6|2023-10-05|Cancelled|      5001|        Card| 25658|\n",
      "|    1002|         11|2023-03-12|  Pending|      5002|      Wallet| 81505|\n",
      "|    1003|         18|2023-04-11|  Pending|      5003|         UPI| 84427|\n",
      "|    1004|          8|2023-02-20|Cancelled|      5004|      Wallet| 43819|\n",
      "|    1005|         18|2023-10-17|Delivered|      5005|  NetBanking| 74728|\n",
      "|    1006|         15|2023-12-31|  Pending|      5006|         UPI| 48462|\n",
      "|    1007|         15|2023-06-27|  Pending|      5007|         UPI| 40836|\n",
      "|    1008|          6|2023-08-25|  Pending|      5008|         UPI| 74756|\n",
      "|    1009|         12|2023-09-12|Delivered|      5009|  NetBanking| 37376|\n",
      "|    1010|         12|2023-03-13|  Pending|      5010|         UPI| 79036|\n",
      "|    1011|         13|2023-03-29|Delivered|      5011|         UPI| 54744|\n",
      "|    1012|          9|2023-04-05|Delivered|      5012|  NetBanking| 16963|\n",
      "|    1013|         12|2023-02-28|Cancelled|      5013|      Wallet| 40016|\n",
      "|    1014|          2|2023-12-08|  Shipped|      5014|        Card| 94866|\n",
      "|    1015|         10|2023-04-17|Delivered|      5015|  NetBanking| 65080|\n",
      "|    1016|          8|2023-04-28|Delivered|      5016|  NetBanking| 77109|\n",
      "|    1017|          5|2023-09-15|  Pending|      5017|        Card| 69613|\n",
      "|    1018|         16|2023-08-01|  Pending|      5018|      Wallet| 50575|\n",
      "|    1019|         11|2023-11-09|  Shipped|      5019|        Card| 10752|\n",
      "|    1020|          7|2023-03-28|Delivered|      5020|        Card| 73679|\n",
      "+--------+-----------+----------+---------+----------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the running total of payments per customer.\n",
    "cust_payment_df = order_df.join(payment_df, 'order_id', 'inner')\n",
    "cust_payment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "525daf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------------+\n",
      "|customer_id|amount|running_total|\n",
      "+-----------+------+-------------+\n",
      "|          2| 94866|        94866|\n",
      "|          5| 69613|        69613|\n",
      "|          6| 25658|        25658|\n",
      "|          6| 74756|       100414|\n",
      "|          7| 73679|        73679|\n",
      "|          8| 43819|        43819|\n",
      "|          8| 77109|       120928|\n",
      "|          9| 16963|        16963|\n",
      "|         10| 65080|        65080|\n",
      "|         11| 10752|        10752|\n",
      "|         11| 81505|        92257|\n",
      "|         12| 37376|        37376|\n",
      "|         12| 40016|        77392|\n",
      "|         12| 79036|       156428|\n",
      "|         13| 54744|        54744|\n",
      "|         15| 40836|        40836|\n",
      "|         15| 48462|        89298|\n",
      "|         16| 50575|        50575|\n",
      "|         18| 74728|        74728|\n",
      "|         18| 84427|       159155|\n",
      "+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#running total find\n",
    "windowSpec = Window.partitionBy('customer_id').orderBy('amount')\n",
    "cust_payment_df.withColumn(\n",
    "    'running_total',\n",
    "    F.sum('amount').over(windowSpec)\n",
    ").select('customer_id', 'amount', 'running_total').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cb20297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_null_df = spark.read.csv('/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/customers_with_nulls.csv', header=True, inferSchema=True)\n",
    "cust_null_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39c09f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----+----------+-------------------+--------------+\n",
      "|customer_id|           name|email|     phone|            address|payment_method|\n",
      "+-----------+---------------+-----+----------+-------------------+--------------+\n",
      "|          6|  Emily Johnson| NULL|9883781479| 54 Main St, City 6|   Credit Card|\n",
      "|          7|   David Wilson| NULL|9773993630|113 Main St, City 8|    Debit Card|\n",
      "|         10|   Olivia Davis| NULL|9655822483| 33 Main St, City 7|           UPI|\n",
      "|         18|Charlotte Lewis| NULL|9744099132| 14 Main St, City 8|        PayPal|\n",
      "+-----------+---------------+-----+----------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_null_df.filter(col('email').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1307b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null payment_method values with \"Unknown\". \n",
    "cust_null_df = cust_null_df.withColumn(\n",
    "    'payment_method_new',\n",
    "    F.when(col('payment_method').isNull(), 'Unknown')\n",
    "    .otherwise(col('payment_method'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8f23380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------------+----------+-------------------+--------------+\n",
      "|customer_id|           name|               email|     phone|            address|payment_method|\n",
      "+-----------+---------------+--------------------+----------+-------------------+--------------+\n",
      "|          1|       John Doe|   john1@hotmail.com|      NULL| 95 Main St, City 8|           UPI|\n",
      "|          2|     Jane Smith|   jane2@outlook.com|9558765521| 22 Main St, City 3|       Unknown|\n",
      "|          3|   Robert Brown| robert3@hotmail.com|9456041729|177 Main St, City 5|   Net Banking|\n",
      "|          4|    Linda White|  linda4@hotmail.com|9792231526| 78 Main St, City 3|       Unknown|\n",
      "|          5|  Michael Green|michael5@outlook.com|9536496600| 28 Main St, City 5|           UPI|\n",
      "|          6|  Emily Johnson|                NULL|9883781479| 54 Main St, City 6|   Credit Card|\n",
      "|          7|   David Wilson|                NULL|9773993630|113 Main St, City 8|    Debit Card|\n",
      "|          8|  Sophia Miller| sophia8@outlook.com|9057322062|136 Main St, City 1|        PayPal|\n",
      "|          9|   James Taylor|    james9@yahoo.com|9250902249|160 Main St, City 5|        PayPal|\n",
      "|         10|   Olivia Davis|                NULL|9655822483| 33 Main St, City 7|           UPI|\n",
      "|         11|  Daniel Thomas|daniel11@hotmail.com|9042475955|141 Main St, City 5|    Debit Card|\n",
      "|         12| Isabella Moore|isabella12@outloo...|      NULL|140 Main St, City 4|           UPI|\n",
      "|         13| Matthew Martin|matthew13@outlook...|9882003372|118 Main St, City 3|           UPI|\n",
      "|         14|    Mia Jackson|   mia14@hotmail.com|9012223780|35 Main St, City 10|       Unknown|\n",
      "|         15|      Ethan Lee| ethan15@outlook.com|      NULL| 74 Main St, City 5|   Credit Card|\n",
      "|         16|  Amelia Harris|  amelia16@gmail.com|9438816950| 26 Main St, City 6|        PayPal|\n",
      "|         17|Alexander Clark|alexander17@hotma...|9124158930| 97 Main St, City 8|    Debit Card|\n",
      "|         18|Charlotte Lewis|                NULL|9744099132| 14 Main St, City 8|        PayPal|\n",
      "|         19|   Henry Walker|   henry19@yahoo.com|9560954126| 43 Main St, City 4|           UPI|\n",
      "|         20|       Ava Hall|   ava20@hotmail.com|9859003609| 81 Main St, City 3|   Credit Card|\n",
      "+-----------+---------------+--------------------+----------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_null_df = cust_null_df.drop('payment_method').withColumnRenamed('payment_method_new', 'payment_method')\n",
    "cust_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5dc913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydata = [\n",
    "    (1, 'x', 23),\n",
    "    (2, 'y', 32)\n",
    "]\n",
    "mycol = ['id', 'name', 'age']\n",
    "\n",
    "mydf = spark.createDataFrame(mydata, mycol)\n",
    "mydf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94f2b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "myschema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "mydf2 = spark.createDataFrame(mydata, myschema)\n",
    "mydf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c53e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.114:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x106577530>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade44200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.printSchema()\n",
    "order_df.printSchema()\n",
    "item_df.printSchema()\n",
    "payment_df.printSchema()\n",
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a739f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 14:45:12 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///Users/harishankargiri/Desktop/Data%20Engineering/PySpark/PractiseByGPT/order_items.csv. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.FileNotFoundException: File file:/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/order_items.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:189)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:572)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4952)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\n",
      "\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\n",
      "\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\n",
      "\tat scala.Option.map(Option.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "26/01/19 14:45:12 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11) (192.168.0.114 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///Users/harishankargiri/Desktop/Data%20Engineering/PySpark/PractiseByGPT/order_items.csv. File does not exist. It is possible the underlying files have been updated.\n",
      "You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.FileNotFoundException: File file:/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/order_items.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:189)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:572)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4952)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\n",
      "\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\n",
      "\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\n",
      "\tat scala.Option.map(Option.scala:242)\n",
      "\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\t... 21 more\n",
      "\n",
      "26/01/19 14:45:12 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///Users/harishankargiri/Desktop/Data%20Engineering/PySpark/PractiseByGPT/order_items.csv. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: java.io.FileNotFoundException: File file:/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/order_items.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:189)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:572)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:1100)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4952)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:100)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\n\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\n\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\n\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\n\tat scala.Option.map(Option.scala:242)\n\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m order_item_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/MyProject/ANACONDA/anaconda3/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[0;32m~/MyProject/ANACONDA/anaconda3/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/MyProject/ANACONDA/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/MyProject/ANACONDA/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/MyProject/ANACONDA/anaconda3/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.FILE_NOT_EXIST] Encountered error while reading file file:///Users/harishankargiri/Desktop/Data%20Engineering/PySpark/PractiseByGPT/order_items.csv. File does not exist. It is possible the underlying files have been updated.\nYou can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistError(QueryExecutionErrors.scala:831)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: java.io.FileNotFoundException: File file:/Users/harishankargiri/Desktop/Data Engineering/PySpark/PractiseByGPT/order_items.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:189)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:572)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:1100)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4952)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:100)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\n\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\n\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\n\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\n\tat scala.Option.map(Option.scala:242)\n\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "order_item_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4996c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
