{"cells": [{"cell_type": "code", "execution_count": 1, "id": "21242038-460b-4908-8854-d704e32bde59", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/05 03:22:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkDataFrame').getOrCreate()"}, {"cell_type": "markdown", "id": "991351e7-8dea-41ac-b231-48ab0ccdaa6c", "metadata": {}, "source": "## read data from local csv"}, {"cell_type": "code", "execution_count": 1, "id": "ea7e4f97-f274-4c50-800e-0d5afea9be94", "metadata": {"tags": []}, "outputs": [], "source": "initial_data = [\n    \"0,Customer_0,Pune,Maharashtra,India,2023-06-29,False\",\n    \"1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\",\n    \"2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True\",\n    \"3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False\",\n    \"4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False\",\n    \"5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False\",\n]\ndata = [row.split(',') for row in initial_data]\ncolumns=[\"ID\", \"Customer\", \"City\", \"State\", \"Country\", \"Date\", \"Active\"]"}, {"cell_type": "code", "execution_count": 2, "id": "14ea2fac-d355-4a53-8f32-3cdc632e1066", "metadata": {"tags": []}, "outputs": [], "source": "df = spark.createDataFrame(data, columns)"}, {"cell_type": "code", "execution_count": 3, "id": "8b60f887-4efa-481c-bdfc-23ba045901c5", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---+----------+---------+-----------+-------+----------+------+\n| ID|  Customer|     City|      State|Country|      Date|Active|\n+---+----------+---------+-----------+-------+----------+------+\n|  0|Customer_0|     Pune|Maharashtra|  India|2023-06-29| False|\n|  1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07|  True|\n|  2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27|  True|\n|  3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17| False|\n|  4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14| False|\n|  5|Customer_5|Hyderabad|  Karnataka|  India|2023-07-28| False|\n+---+----------+---------+-----------+-------+----------+------+\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": 20, "id": "179ad703-044a-450a-8bf7-ee36ceb5225a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- ID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Active: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "markdown", "id": "135111ac-6474-4be1-aa5b-30e6a40d9c92", "metadata": {}, "source": "## read data from HDFS storage"}, {"cell_type": "code", "execution_count": 13, "id": "4d932712-6299-44f5-8c1c-24efdf30ed79", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\n-rw-r--r--   2 root hadoop    1048576 2025-06-03 03:00 /data/customers10.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-05 03:18 /data/customers150.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-04 03:32 /data/customers500.csv\n"}], "source": "!hdfs dfs -ls /data/"}, {"cell_type": "code", "execution_count": 16, "id": "43c17ace-681a-4da0-80b8-92e0cb152780", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-rw-r--r--   2 root hadoop        1 M 2025-06-05 03:18 /data/customers150.csv\n"}], "source": "!hdfs dfs -ls -h /data/customers150.csv"}, {"cell_type": "code", "execution_count": 17, "id": "ca957f29-d8bb-4528-8c62-4c67708c66b5", "metadata": {"tags": []}, "outputs": [], "source": "path = '/data/customers150.csv'\n# Read the CSV file\ndf2 = spark.read.format('csv') \\\n    .option('header', 'true') \\\n    .option('inferSchema', 'true') \\\n    .load(path)"}, {"cell_type": "code", "execution_count": 18, "id": "313e7394-7728-4c2d-87ed-89bf7d7ba836", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+---------------+---------+-----------+-----+----------+-----+\n|  5919|Customer_135919|    Delhi|  Telangana|India|2023-05-02|False|\n+------+---------------+---------+-----------+-----+----------+-----+\n|135920|Customer_135920|  Chennai|    Gujarat|India|2023-01-06| true|\n|135921|Customer_135921|    Delhi|  Telangana|India|2023-01-30|false|\n|135922|Customer_135922|  Chennai|    Gujarat|India|2023-11-09|false|\n|135923|Customer_135923|  Chennai|      Delhi|India|2023-04-27|false|\n|135924|Customer_135924|  Kolkata| Tamil Nadu|India|2023-08-18|false|\n|135925|Customer_135925|Ahmedabad|  Karnataka|India|2023-09-12| true|\n|135926|Customer_135926|Bangalore|Maharashtra|India|2023-03-05| true|\n|135927|Customer_135927|  Kolkata|Maharashtra|India|2023-09-30|false|\n|135928|Customer_135928|Hyderabad|Maharashtra|India|2023-03-27| true|\n|135929|Customer_135929|     Pune|West Bengal|India|2023-03-25| true|\n|135930|Customer_135930|Bangalore| Tamil Nadu|India|2023-11-01| true|\n|135931|Customer_135931|  Kolkata|West Bengal|India|2023-09-19|false|\n|135932|Customer_135932|    Delhi| Tamil Nadu|India|2023-04-12|false|\n|135933|Customer_135933|  Kolkata|  Telangana|India|2023-09-25|false|\n|135934|Customer_135934|    Delhi|  Karnataka|India|2023-06-30| true|\n|135935|Customer_135935|Hyderabad|    Gujarat|India|2023-11-17| true|\n|135936|Customer_135936|Hyderabad|Maharashtra|India|2023-02-16|false|\n|135937|Customer_135937|Hyderabad|    Gujarat|India|2023-03-08| true|\n|135938|Customer_135938|    Delhi|    Gujarat|India|2023-10-12|false|\n|135939|Customer_135939|  Kolkata| Tamil Nadu|India|2023-08-28|false|\n+------+---------------+---------+-----------+-----+----------+-----+\nonly showing top 20 rows\n\n"}], "source": "df2.show()"}, {"cell_type": "code", "execution_count": 19, "id": "92ac6309-369c-498c-9c9c-ea98a2eb9bbe", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- 5919: integer (nullable = true)\n |-- Customer_135919: string (nullable = true)\n |-- Delhi: string (nullable = true)\n |-- Telangana: string (nullable = true)\n |-- India: string (nullable = true)\n |-- 2023-05-02: date (nullable = true)\n |-- False: boolean (nullable = true)\n\n"}], "source": "df2.printSchema()"}, {"cell_type": "code", "execution_count": 20, "id": "43af0421-4311-48b3-8ba4-c717602b5a40", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, BooleanType, StringType\nschema = StructType([\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False),\n    StructField( 'ID', IntegerType(),False)\n])"}, {"cell_type": "code", "execution_count": null, "id": "20775dab-dd7b-4ca2-8ecf-3b2605edebc1", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}