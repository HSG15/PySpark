{"cells": [{"cell_type": "code", "execution_count": 7, "id": "df92eb3e-7757-4cda-9f67-6a5011b367cc", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkDataFrame').getOrCreate()"}, {"cell_type": "code", "execution_count": 18, "id": "fce4b165-2744-46b9-a2d8-f16f73fe0cdb", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\n-rw-r--r--   2 root hadoop    1048576 2025-06-03 03:00 /data/customers10.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-05 03:18 /data/customers150.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-04 03:32 /data/customers500.csv\n-rw-r--r--   2 root hadoop        511 2025-06-06 02:33 /data/malformed_customers.csv\n"}], "source": "!hdfs dfs -ls /data/"}, {"cell_type": "code", "execution_count": 36, "id": "8ba41dab-35b6-4d51-8c26-2d9364cdbe0e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "id,name,age,is_active,date\n1,John,25,true,2024-06-01\n2,Alice,30,false,2024-06-02\n3,Bob,twenty,true,2024-06-03\n4,Charlie,40,,2024-06-04\n5,David,abc,true,not_a_date\n6,Eve,,false,2024-06-06\n7,,28,true,2024-06-07\n8,Frank,35,yes,2024-06-08\n9,Gina,45,true,2024-06-09\n10,Hank,28,false,\n11,Ivy,29,,2024-06-11\n12,,thirty,false,2024-06-12\n13,Jack,32,maybe,2024-06-13\n14,Kim,27,true,\n15,Luke,33,false,2024-06-15\n16,Mia,31,true,2024-06-16\n17,Nina,27,false,2024-06-17\n18,Oscar,NaN,true,2024-06-18\n19,Paul,29,true,2024-06-19\n"}], "source": "!hdfs dfs -cat /data/malformed_customers.csv"}, {"cell_type": "code", "execution_count": 17, "id": "3ff41ef5-7869-48c7-8456-901f7566cd49", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "0"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "import pandas as pd\n\n# Create your pandas DataFrame\ndata = [\n    [1, \"John\", 25, \"true\", \"2024-06-01\"],\n    [2, \"Alice\", 30, \"false\", \"2024-06-02\"],\n    [3, \"Bob\", \"twenty\", \"true\", \"2024-06-03\"],\n    [4, \"Charlie\", 40, None, \"2024-06-04\"],\n    [5, \"David\", \"abc\", \"true\", \"not_a_date\"],\n    [6, \"Eve\", None, \"false\", \"2024-06-06\"],\n    [7, None, 28, \"true\", \"2024-06-07\"],\n    [8, \"Frank\", 35, \"yes\", \"2024-06-08\"],\n    [9, \"Gina\", 45, \"true\", \"2024-06-09\"],\n    [10, \"Hank\", 28, \"false\", None],\n    [11, \"Ivy\", 29, None, \"2024-06-11\"],\n    [12, None, \"thirty\", \"false\", \"2024-06-12\"],\n    [13, \"Jack\", 32, \"maybe\", \"2024-06-13\"],\n    [14, \"Kim\", 27, \"true\", None],\n    [15, \"Luke\", 33, \"false\", \"2024-06-15\"],\n    [16, \"Mia\", 31, \"true\", \"2024-06-16\"],\n    [17, \"Nina\", 27, \"false\", \"2024-06-17\"],\n    [18, \"Oscar\", \"NaN\", \"true\", \"2024-06-18\"],\n    [19, \"Paul\", 29, \"true\", \"2024-06-19\"]\n]\ncolumns = [\"id\", \"name\", \"age\", \"is_active\", \"date\"]\n\npdf = pd.DataFrame(data, columns=columns)\n\n# Save it locally\nlocal_path = \"/tmp/my_customers.csv\"\npdf.to_csv(local_path, index=False)\n\n# Use HDFS command-line to put it in HDFS\nimport os\nos.system(f\"hdfs dfs -put -f {local_path} /data/malformed_customers.csv\")\n"}, {"cell_type": "code", "execution_count": 60, "id": "b1ea1113-5d9f-42e9-a449-f1ad7835dafa", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\nschema = StructType([\n    StructField('id', IntegerType(), True),\n    StructField('name', StringType(), True),\n    StructField('age', IntegerType(), True),\n    StructField('is_active', BooleanType(), True),\n    StructField('date', StringType(), True)\n])\n\n# If we wont do schema enforcement, then spark can read the malformed data and won't show any error and won't skip any data as well\n# Without schema enforcement, Spark just reads everything as strings, and it doesn\u2019t know anything\u2019s \u201cmalformed\u201d!"}, {"cell_type": "code", "execution_count": 61, "id": "079207dc-b5a6-4fb3-8055-c565f418c847", "metadata": {"tags": []}, "outputs": [], "source": "df = spark.read.format('csv').option('header', True).schema(schema).option('mode', 'DROPMALFORMED').load('/data/malformed_customers.csv')"}, {"cell_type": "code", "execution_count": 62, "id": "4a2b72a1-dfba-4280-80e7-865c9d3f122c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+----+---------+----------+\n| id|   name| age|is_active|      date|\n+---+-------+----+---------+----------+\n|  1|   John|  25|     true|2024-06-01|\n|  2|  Alice|  30|    false|2024-06-02|\n|  4|Charlie|  40|     NULL|2024-06-04|\n|  6|    Eve|NULL|    false|2024-06-06|\n|  7|   NULL|  28|     true|2024-06-07|\n|  9|   Gina|  45|     true|2024-06-09|\n| 10|   Hank|  28|    false|      NULL|\n| 11|    Ivy|  29|     NULL|2024-06-11|\n| 14|    Kim|  27|     true|      NULL|\n| 15|   Luke|  33|    false|2024-06-15|\n| 16|    Mia|  31|     true|2024-06-16|\n| 17|   Nina|  27|    false|2024-06-17|\n| 19|   Paul|  29|     true|2024-06-19|\n+---+-------+----+---------+----------+\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": 63, "id": "9bd568bb-0110-4e28-bdbd-cad12d30438a", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "markdown", "id": "7644e979-150c-48e1-8ed7-7574bfd4abb3", "metadata": {}, "source": "# Spark Read Modes\n\n| Mode          | Behavior                                      | When to use                           |\n| ------------- | --------------------------------------------- | ------------------------------------- |\n| PERMISSIVE    | Loads everything; bad records get `null`      | Load as much data as possible         |\n| DROPMALFORMED | Skips malformed rows entirely                 | Ignore bad data and load good records |\n| FAILFAST      | Throws an error and stops reading immediately | Ensure data is 100% clean and valid   |"}, {"cell_type": "code", "execution_count": null, "id": "07ada042-870f-42b6-a150-2567ff165811", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}