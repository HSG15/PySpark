{"cells": [{"cell_type": "code", "execution_count": 1, "id": "1ca30040-3193-4879-a518-2ae5b2502a2e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/07 07:49:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkDataFrame').getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "b6d371a7-c30c-40a4-b2e8-c05487354d37", "metadata": {"tags": []}, "outputs": [], "source": "data = [\n    [1, \"John\", 25, \"true\", \"2024-06-01\"],\n    [2, \"Alice\", 30, \"false\", \"2024-06-02\"],\n    [3, \"Bob\", 20, \"true\", \"2024-06-03\"],\n    [4, \"Paul\", 29, \"true\", \"2024-06-19\"]\n]\ncolumns = [\"id\", \"name\", \"age\", \"is_active\", \"date\"]"}, {"cell_type": "code", "execution_count": 3, "id": "0dcdf4f7-859e-4f6d-9299-bc00cc955a6c", "metadata": {"tags": []}, "outputs": [], "source": "df = spark.createDataFrame(data, columns)"}, {"cell_type": "code", "execution_count": 4, "id": "f4215b46-fe1e-4b8b-9b6d-b7450ba8f258", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "df.rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 21, "id": "086f81a7-4208-46a6-b9a8-0fe66126addd", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\n-rw-r--r--   2 root hadoop    1048576 2025-06-03 03:00 /data/customers10.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-05 03:18 /data/customers150.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-04 03:32 /data/customers500.csv\n-rw-r--r--   2 root hadoop        511 2025-06-06 02:33 /data/malformed_customers.csv\n"}], "source": "!hdfs dfs -ls /data/"}, {"cell_type": "code", "execution_count": 6, "id": "cebd1e4e-ec48-4008-8818-f146fadeceb9", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-first-cluster-m.us-central1-f.c.striking-berm-457204-i1.internal:34371\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f925667f550>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 7, "id": "9d078dc6-97a1-444f-a708-311664c75dc0", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[id: bigint, name: string, age: bigint, is_active: string, date: string]"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "df"}, {"cell_type": "code", "execution_count": 22, "id": "5433a6d5-430d-4b43-a3fd-9da592b49659", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.write.format('csv').option('header', 'true').option('delimiter', '|').save('/data/write_op')"}, {"cell_type": "code", "execution_count": 23, "id": "daae1594-2c4b-4637-89c8-8b7100243b02", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 5 items\n-rw-r--r--   2 root hadoop    1048576 2025-06-03 03:00 /data/customers10.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-05 03:18 /data/customers150.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-04 03:32 /data/customers500.csv\n-rw-r--r--   2 root hadoop        511 2025-06-06 02:33 /data/malformed_customers.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-07 08:08 /data/write_op\n"}], "source": "!hdfs dfs -ls /data/"}, {"cell_type": "code", "execution_count": 26, "id": "22baf832-2d92-48f4-8a99-d0e466aee9b1", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "id|name|age|is_active|date\n1|John|25|true|2024-06-01\n2|Alice|30|false|2024-06-02\nid|name|age|is_active|date\n3|Bob|20|true|2024-06-03\n4|Paul|29|true|2024-06-19\n"}], "source": "!hdfs dfs -cat /data/write_op/*"}, {"cell_type": "markdown", "id": "03642c2e-7dce-416e-abfc-db476e4efd63", "metadata": {}, "source": "#### Here we are getting 2 different output as getNumPartitions() = 2, To get one output repartition the rdd to 1."}, {"cell_type": "code", "execution_count": 31, "id": "4ec6035c-5122-4946-9fcb-83ab368a3118", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "spark stopppped\n"}], "source": "try:\n    spark.stop()\n    print('spark stopppped')\nexcept e as Exception:\n    print(e)"}, {"cell_type": "code", "execution_count": null, "id": "4cb09c83-c3d3-48c0-bc46-23762a659541", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}