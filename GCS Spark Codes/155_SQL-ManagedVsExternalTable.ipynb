{"cells": [{"cell_type": "code", "execution_count": 1, "id": "68f269e7-4b5c-4ae1-8f06-fe1c929462dc", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/10 02:41:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SQL Tables').getOrCreate()"}, {"cell_type": "markdown", "id": "96f605a9-477f-4f73-90a5-04d4645411b4", "metadata": {}, "source": "### Managed Table"}, {"cell_type": "code", "execution_count": 43, "id": "a820a6f8-0f4e-47c9-a5b8-c86a6a1fa291", "metadata": {"tags": []}, "outputs": [], "source": "df = spark.read.format('csv').option('inferScema', True).option('header', True).load('/data/customers1.csv')"}, {"cell_type": "code", "execution_count": 44, "id": "33395f2d-fd9b-4c2a-babc-1d42e975b8de", "metadata": {"tags": []}, "outputs": [], "source": "df2 = df.withColumnRenamed(\"2023-10-07\", \"date\")"}, {"cell_type": "code", "execution_count": 45, "id": "63756886-e2c5-44a9-bd59-c21d9b6b9a66", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+--------------+---------+\n|India|          date|    False|\n+-----+--------------+---------+\n|17455|Customer_17455|    Delhi|\n|17456|Customer_17456|Bangalore|\n|17457|Customer_17457|  Kolkata|\n|17458|Customer_17458|Hyderabad|\n|17459|Customer_17459|Bangalore|\n|17460|Customer_17460|     Pune|\n|17461|Customer_17461|  Chennai|\n|17462|Customer_17462|Bangalore|\n|17463|Customer_17463|   Mumbai|\n|17464|Customer_17464|  Kolkata|\n|17465|Customer_17465|   Mumbai|\n|17466|Customer_17466|   Mumbai|\n|17467|Customer_17467|Bangalore|\n|17468|Customer_17468|Hyderabad|\n|17469|Customer_17469|Bangalore|\n|17470|Customer_17470|Hyderabad|\n|17471|Customer_17471|    Delhi|\n|17472|Customer_17472|Hyderabad|\n|17473|Customer_17473|Ahmedabad|\n|17474|Customer_17474|   Mumbai|\n+-----+--------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df2.show()"}, {"cell_type": "code", "execution_count": 46, "id": "5b4d07fe-cd2a-4f3a-a2b2-bed09e8fd353", "metadata": {"tags": []}, "outputs": [], "source": "df2.createOrReplaceTempView('manage_cust')"}, {"cell_type": "code", "execution_count": 47, "id": "a561cb67-2228-4d90-9a69-bd5487a746c0", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- India: string (nullable = true)\n |-- date: string (nullable = true)\n |-- False: string (nullable = true)\n\n"}], "source": "df2.printSchema()"}, {"cell_type": "code", "execution_count": 48, "id": "ccadf75d-a474-4f4b-93d9-ecf833958aa6", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+--------------+---------+\n|India|          date|    False|\n+-----+--------------+---------+\n|17455|Customer_17455|    Delhi|\n|17456|Customer_17456|Bangalore|\n|17457|Customer_17457|  Kolkata|\n|17458|Customer_17458|Hyderabad|\n|17459|Customer_17459|Bangalore|\n+-----+--------------+---------+\n\n"}], "source": "spark.sql('select * from manage_cust limit 5').show()"}, {"cell_type": "code", "execution_count": 51, "id": "5f470521-79f1-452f-ab58-37a7cc740ee8", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 51, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('drop table if exists managed_customers')"}, {"cell_type": "code", "execution_count": 52, "id": "5002eb97-bbdc-46c4-a75b-41d2561819e2", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/10 04:48:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"}, {"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('''\n    create table managed_customers \n    as \n    select * from manage_cust\n''').show()"}, {"cell_type": "code", "execution_count": 53, "id": "3155fe3f-7235-48d1-a852-3cf4feb9a120", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|               India|              string|   NULL|\n|                date|              string|   NULL|\n|               False|              string|   NULL|\n|                    |                    |       |\n|# Detailed Table ...|                    |       |\n|             Catalog|       spark_catalog|       |\n|            Database|             default|       |\n|               Table|   managed_customers|       |\n|               Owner|                root|       |\n|        Created Time|Tue Jun 10 04:48:...|       |\n|         Last Access|             UNKNOWN|       |\n|          Created By|         Spark 3.5.3|       |\n|                Type|             MANAGED|       |\n|            Provider|                hive|       |\n|    Table Properties|[transient_lastDd...|       |\n|          Statistics|          5753 bytes|       |\n|            Location|hdfs://my-first-c...|       |\n|       Serde Library|org.apache.hadoop...|       |\n|         InputFormat|org.apache.hadoop...|       |\n|        OutputFormat|org.apache.hadoop...|       |\n+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended managed_customers').show()"}, {"cell_type": "markdown", "id": "5fa29dbe-2bad-4ce2-8508-ae8afe084489", "metadata": {}, "source": "### External Table"}, {"cell_type": "code", "execution_count": 21, "id": "8dcf15a1-1a20-4414-88d2-7b47195da0b4", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 6 items\n-rw-r--r--   2 root hadoop      12174 2025-06-09 03:26 /data/customers1.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-03 03:00 /data/customers10.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-05 03:18 /data/customers150.csv\n-rw-r--r--   2 root hadoop    1048576 2025-06-04 03:32 /data/customers500.csv\n-rw-r--r--   2 root hadoop        511 2025-06-06 02:33 /data/malformed_customers.csv\ndrwxr-xr-x   - root hadoop          0 2025-06-07 08:08 /data/write_op\n"}], "source": "!hdfs dfs -ls /data/"}, {"cell_type": "code", "execution_count": 55, "id": "fbe52f1f-5d62-472a-909e-904cc93bd269", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 55, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('drop table external_customers')"}, {"cell_type": "code", "execution_count": 64, "id": "7b69e8ce-b87d-4666-8062-9204b0f35371", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/10 05:03:27 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider CSV. Persisting data source table `spark_catalog`.`default`.`external_customers2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 64, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('''\n    CREATE EXTERNAL TABLE external_customers2(\n         India string,\n         date string,\n         False string\n    )\n    USING CSV\n    LOCATION '/data/external_data/'\n''')"}, {"cell_type": "code", "execution_count": 57, "id": "ef017e39-d371-46ba-ac0d-2c1dfbe980c6", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|                 _c0|              string|   NULL|\n|                 _c1|              string|   NULL|\n|                 _c2|              string|   NULL|\n|                    |                    |       |\n|# Detailed Table ...|                    |       |\n|             Catalog|       spark_catalog|       |\n|            Database|             default|       |\n|               Table|  external_customers|       |\n|               Owner|                root|       |\n|        Created Time|Tue Jun 10 04:54:...|       |\n|         Last Access|             UNKNOWN|       |\n|          Created By|         Spark 3.5.3|       |\n|                Type|            EXTERNAL|       |\n|            Provider|                 CSV|       |\n|            Location|hdfs://my-first-c...|       |\n|       Serde Library|org.apache.hadoop...|       |\n|         InputFormat|org.apache.hadoop...|       |\n|        OutputFormat|org.apache.hadoop...|       |\n+--------------------+--------------------+-------+\n\n"}], "source": "spark.sql('describe extended external_customers').show()"}, {"cell_type": "code", "execution_count": 61, "id": "54b39e6d-4a8b-4728-b8ff-2f0d64e30d5c", "metadata": {"tags": []}, "outputs": [], "source": "!hdfs dfs -mkdir /data/external_data"}, {"cell_type": "code", "execution_count": 62, "id": "d738d508-3b2d-4671-af41-a0079fa59914", "metadata": {"tags": []}, "outputs": [], "source": "!hdfs dfs -cp /data/customers1.csv /data/external_data/"}, {"cell_type": "code", "execution_count": 63, "id": "4dbe57a3-8f22-472d-8ee6-69eb5bd803bc", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\n-rw-r--r--   2 root hadoop      12174 2025-06-10 05:01 /data/external_data/customers1.csv\n"}], "source": "!hdfs dfs -ls /data/external_data/"}, {"cell_type": "code", "execution_count": null, "id": "bae8fbbf-0eb3-4380-bf54-3d0c70cf85b5", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}