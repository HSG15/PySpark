{"cells": [{"cell_type": "code", "execution_count": 1, "id": "7949ccb7-ce77-4da1-8874-edf5a36c0cd6", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "0fd0ba4d-724c-40bf-badd-a59c843f79e0", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-first-cluster-m.us-central1-f.c.striking-berm-457204-i1.internal:37683\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f55e85af590>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "id": "ef351738-05ed-4e4f-9cc0-41077a8418bf", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/27 03:19:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder.appName('Word Count App').master('yarn').getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "f4c683bd-cd61-4492-bce3-3959ab4ea390", "metadata": {"tags": []}, "outputs": [], "source": "sc = spark.sparkContext"}, {"cell_type": "markdown", "id": "d0e822dd-c296-4053-aeb4-2744dc38b4ae", "metadata": {}, "source": "Execute below commands to put the input in file in HDFS via terminal (GCP Cluster):\n\ncd tmp\n\nvi inputhdfsdbz.txt\n\nhadoop fs -put inputhdfsdbz.txt /tmp/          : Now we can see it in HDFS NameNode"}, {"cell_type": "code", "execution_count": 6, "id": "98e58996-dcc5-4e05-8f96-d350066c0d68", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\ndrwxrwxrwt   - hdfs hadoop          0 2025-04-18 06:05 /tmp/hadoop-yarn\ndrwx-wx-wx   - hive hadoop          0 2025-04-18 06:05 /tmp/hive\n-rw-r--r--   2 root hadoop         42 2025-04-24 04:12 /tmp/inputdbztest.txt\n"}], "source": "!hadoop fs -ls -h /tmp/"}, {"cell_type": "code", "execution_count": 7, "id": "aabce1df-48de-4041-ada7-2b3f95148fa6", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = '/tmp/inputdbztest.txt'"}, {"cell_type": "code", "execution_count": 8, "id": "727d9924-5c8b-49ff-bec0-3ed22b6c8aaa", "metadata": {"tags": []}, "outputs": [], "source": "rdd = sc.textFile(hdfs_path)"}, {"cell_type": "code", "execution_count": 10, "id": "c699843f-d1f5-4560-b874-0c7146cb7e17", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 13, "id": "97530f89-48dd-4557-9e3d-5d6d982fd1d7", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "['dear bear car', 'car car bear', 'dear dear bear']"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "rdd.collect()"}, {"cell_type": "code", "execution_count": 14, "id": "8c1bdac9-bc97-4bf1-9ff3-d4f96067202f", "metadata": {"tags": []}, "outputs": [], "source": "flatMappedData = rdd.flatMap(lambda word : word.split())"}, {"cell_type": "code", "execution_count": 15, "id": "6285eae5-01f5-47fc-ac9c-a1ce2d3ce727", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['dear', 'bear', 'car', 'car', 'car', 'bear', 'dear', 'dear', 'bear']"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "flatMappedData.collect()"}, {"cell_type": "code", "execution_count": 17, "id": "8f149610-2b65-4910-92b2-e2b6e3f176d9", "metadata": {"tags": []}, "outputs": [], "source": "mappedData = flatMappedData.map(lambda x : (x, 1))"}, {"cell_type": "code", "execution_count": 18, "id": "bdbad874-d20d-44a1-b621-73e049bdf964", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('dear', 1),\n ('bear', 1),\n ('car', 1),\n ('car', 1),\n ('car', 1),\n ('bear', 1),\n ('dear', 1),\n ('dear', 1),\n ('bear', 1)]"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "mappedData.collect()"}, {"cell_type": "code", "execution_count": 19, "id": "8c2da269-7fe2-4057-b613-cd790108f6d8", "metadata": {"tags": []}, "outputs": [], "source": "reducedData = mappedData.reduceByKey(lambda a,b : a+b)"}, {"cell_type": "code", "execution_count": 21, "id": "6f728e1b-440a-41dc-8353-6721b8a302a1", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[('bear', 3), ('car', 3), ('dear', 3)]"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "reducedData.collect()"}, {"cell_type": "code", "execution_count": 16, "id": "3e821380-452c-4dcc-82fe-615b5cdc15e0", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Max Partition Bytes: 134217728b\n"}], "source": "print(f\"Max Partition Bytes: {spark.conf.get('spark.sql.files.maxPartitionBytes')}\")"}, {"cell_type": "code", "execution_count": 19, "id": "a4844130-a99c-443f-a844-3986f340cbcb", "metadata": {"tags": []}, "outputs": [{"ename": "AttributeError", "evalue": "property 'defaultParallelism' of 'SparkContext' object has no setter", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n", "\u001b[0;31mAttributeError\u001b[0m: property 'defaultParallelism' of 'SparkContext' object has no setter"]}], "source": "sc.defaultParallelism = 1"}, {"cell_type": "code", "execution_count": null, "id": "d0668181-4b73-4cde-b135-51b861b3089f", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}